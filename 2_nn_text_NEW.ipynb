{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayKuraga/ML_lab_2/blob/master/2_nn_text_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt738_1Qi4Nd",
        "outputId": "db3de21c-8afc-4d11-9814-de180c5ad098"
      },
      "id": "vt738_1Qi4Nd",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ee1b0f88-b8c8-4b31-aec2-575ffe7b387b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1b0f88-b8c8-4b31-aec2-575ffe7b387b",
        "outputId": "7d0443f0-19ee-438c-b3a2-f4756e0cb39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WOW! I can execute shell commands from Jupyter Notebook!\n",
            "Interesting, what shell is used under Windows...\n",
            "Current shell is: \"/bin/bash\".\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!echo 'WOW! I can execute shell commands from Jupyter Notebook!'\n",
        "!echo 'Interesting, what shell is used under Windows...'\n",
        "!echo \"Current shell is: \\\"${SHELL}\\\".\"\n",
        "!pip install fast_langdetect iso-639 nltk pandas >/dev/null\n",
        "\n",
        "\n",
        "import enum\n",
        "import string\n",
        "import typing as t\n",
        "\n",
        "\n",
        "import fast_langdetect\n",
        "import iso639\n",
        "import nltk.corpus\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3519c42-4cca-439e-a268-f5ba24cbd1c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3519c42-4cca-439e-a268-f5ba24cbd1c0",
        "outputId": "c9b3a36a-ae53-4931-cf78-441fc7dbf17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of train dataset:\n",
            "   Class Index                                              Title  \\\n",
            "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
            "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
            "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
            "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
            "4            3  Oil prices soar to all-time record, posing new...   \n",
            "\n",
            "                                         Description  \n",
            "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
            "1  Reuters - Private investment firm Carlyle Grou...  \n",
            "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
            "3  Reuters - Authorities have halted oil export\\f...  \n",
            "4  AFP - Tearaway world oil prices, toppling reco...  \n",
            "\n",
            "Info of train dataset\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120000 entries, 0 to 119999\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   Class Index  120000 non-null  int64 \n",
            " 1   Title        120000 non-null  object\n",
            " 2   Description  120000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.7+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "df_trn = pd.read_csv('https://raw.githubusercontent.com/NikolayKuraga/ML_lab_2/refs/heads/master/train.csv')\n",
        "df_tst = pd.read_csv('https://raw.githubusercontent.com/NikolayKuraga/ML_lab_2/refs/heads/master/test.csv')\n",
        "\n",
        "print('Head of train dataset:')\n",
        "print(df_trn.head())\n",
        "print()\n",
        "print('Info of train dataset')\n",
        "print(df_trn.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Try some text cleaning techniques from the list below (at least 2). Does any of it improved your model quality? Try to assume why."
      ],
      "metadata": {
        "id": "jtC5UmmyYyoe"
      },
      "id": "jtC5UmmyYyoe"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4dd55ef6-32a6-4b52-a725-43c656d74377",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dd55ef6-32a6-4b52-a725-43c656d74377",
        "outputId": "06fb9ec4-4446-4b82-f702-14036d4d4a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset\n",
            "Lang\n",
            "english           119944\n",
            "french                21\n",
            "spanish               10\n",
            "polish                 9\n",
            "german                 7\n",
            "italian                5\n",
            "westernfrisian         3\n",
            "ukrainian              1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test dataset\n",
            "Lang\n",
            "english    7596\n",
            "polish        3\n",
            "french        1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1. Try some text cleaning techniques from the list below (at least 2). Does any of it\n",
        "# improved your model quality? Try to assume why.\n",
        "#     a. stop words removing\n",
        "#     b. punctuation removing\n",
        "#     c. Trash (Extra spaces / special symbols (like @#~<> etc)) removing\n",
        "#     d. digits removing\n",
        "\n",
        "\n",
        "# First of all tried to detect what languages are used.\n",
        "# Found two ways:\n",
        "#     1) langdetect -- \"traditional\" module, really slow (several seconds for 1000 entities).\n",
        "#     2) fast_langdetect -- fast \"zoomer\" language-machine-learning based module.\n",
        "# I will use fast_langdetect model 'cause I'm zoomer.\n",
        "\n",
        "def detectLanguage(text: str) -> str:\n",
        "    code = fast_langdetect.detect_language(text).lower()\n",
        "    name = iso639.languages.get(alpha2=code).name\n",
        "    name = ''.join(name.split()).lower()\n",
        "    return name\n",
        "\n",
        "\n",
        "df_trn['Lang'] = (df_trn['Title'] + ' ' + df_trn['Description']).apply(detectLanguage)\n",
        "df_tst['Lang'] = (df_tst['Title'] + ' ' + df_tst['Description']).apply(detectLanguage)\n",
        "\n",
        "print('Train dataset')\n",
        "print(df_trn['Lang'].value_counts())\n",
        "print()\n",
        "print('Test dataset')\n",
        "print(df_tst['Lang'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4306691c-7a1a-4a0f-859c-3d9d47da3e39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4306691c-7a1a-4a0f-859c-3d9d47da3e39",
        "outputId": "625ed780-8546-4e10-d4d8-e0ba6b21a8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no \"polish\" language in stopwords database?\n",
            "no \"ukrainian\" language in stopwords database?\n",
            "no \"westernfrisian\" language in stopwords database?\n",
            "\n",
            "Got so many (1089) stopwords from different languages!\n",
            "\n",
            "35                                  Steady as they go\n",
            "36         Google IPO: Type in 'confusing,' 'secrecy'\n",
            "37                        A bargain hunter's paradise\n",
            "38     Researchers seek to untangle the e-mail thread\n",
            "39    Microsoft Corp. 2.0: a kinder corporate culture\n",
            "Name: Title, dtype: object\n",
            "\n",
            "35                                     steady go\n",
            "36             google ipo type confusing secrecy\n",
            "37                      bargain hunters paradise\n",
            "38        researchers seek untangle email thread\n",
            "39    microsoft corp 20 kinder corporate culture\n",
            "Name: Title no stopwords8punc, dtype: object\n",
            "\n",
            "35                                  steady go\n",
            "36          google ipo type confusing secrecy\n",
            "37                   bargain hunters paradise\n",
            "38     researchers seek untangle email thread\n",
            "39    microsoft corp kinder corporate culture\n",
            "Name: Title no stopwords8punc8dig, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Find necessary stopwords in nltk database.\n",
        "stopwords = set()\n",
        "for lang in df_trn['Lang'].unique():\n",
        "    try:\n",
        "        stopwords |= set(nltk.corpus.stopwords.words(lang))\n",
        "    except:\n",
        "        print(f'no \"{lang}\" language in stopwords database?')\n",
        "\n",
        "print()\n",
        "print(f'Got so many ({len(stopwords)}) stopwords from different languages!')\n",
        "\n",
        "def cleanTextFromStopwords8Punc(txt: str) -> str:\n",
        "    txt = ''.join([sym for sym in txt if sym not in string.punctuation])\n",
        "    txt = ' '.join([wrd for wrd in txt.lower().split() if wrd not in stopwords])\n",
        "    return txt\n",
        "\n",
        "def cleanTextFromStopwords8Punc8Dig(txt: str) -> str:\n",
        "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
        "    txt = ''.join([sym for sym in txt if sym not in string.punctuation])\n",
        "    txt = ' '.join([wrd for wrd in txt.lower().split() if wrd not in stopwords])\n",
        "    return txt\n",
        "\n",
        "df_trn['Title no stopwords8punc'] = df_trn['Title'].apply(cleanTextFromStopwords8Punc)\n",
        "df_trn['Title no stopwords8punc8dig'] = df_trn['Title'].apply(cleanTextFromStopwords8Punc8Dig) #Title no stopwords8punc8dig\n",
        "df_trn['Description no stopwords8punc'] = df_trn['Description'].apply(cleanTextFromStopwords8Punc) #Description no stopwords8punc\n",
        "df_trn['Description no stopwords8punc8dig'] = df_trn['Description'].apply(cleanTextFromStopwords8Punc8Dig)\n",
        "\n",
        "df_tst['Title no stopwords8punc'] = df_tst['Title'].apply(cleanTextFromStopwords8Punc)\n",
        "df_tst['Title no stopwords8punc8dig'] = df_tst['Title'].apply(cleanTextFromStopwords8Punc8Dig)\n",
        "df_tst['Description no stopwords8punc'] = df_tst['Description'].apply(cleanTextFromStopwords8Punc)\n",
        "df_tst['Description no stopwords8punc8dig'] = df_tst['Description'].apply(cleanTextFromStopwords8Punc8Dig)\n",
        "\n",
        "print()\n",
        "print(df_trn['Title'].iloc[35:40])\n",
        "print()\n",
        "print(df_trn['Title no stopwords8punc'].iloc[35:40])\n",
        "print()\n",
        "print(df_trn['Title no stopwords8punc8dig'].iloc[35:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Try to apply stemming and lemmatization."
      ],
      "metadata": {
        "id": "avgumQcoY3Mt"
      },
      "id": "avgumQcoY3Mt"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train =  df_trn[['Title no stopwords8punc8dig','Description no stopwords8punc8dig']]\n",
        "y_train =  df_trn['Class Index']\n",
        "\n",
        "X_test =  df_tst[['Title no stopwords8punc8dig','Description no stopwords8punc8dig']]"
      ],
      "metadata": {
        "id": "kZm4glhaYa7A"
      },
      "id": "kZm4glhaYa7A",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatzation(lst):\n",
        "   new_lem = []\n",
        "   for i in lst:\n",
        "       i = lemmatizer.lemmatize(i)\n",
        "       new_lem.append(i)\n",
        "   return new_lem\n",
        "\n",
        "train_x_lem = X_train.apply(lemmatzation)\n",
        "test_x_lem = X_test.apply(lemmatzation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCxnr7iTYtKm",
        "outputId": "5f487968-34a8-4f4b-ca97-61c806d7fad0"
      },
      "id": "iCxnr7iTYtKm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(txt):\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w) for w in txt.split()]  # Лемматизация\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "# Применим лемматизацию к каждому столбцу в тест и трейн\n",
        "train_x_lem = pd.DataFrame()\n",
        "train_x_lem['title'] = X_train['Title no stopwords8punc8dig'].apply(lemmatize_text)\n",
        "train_x_lem['description'] = X_train['Description no stopwords8punc8dig'].apply(lemmatize_text)\n",
        "\n",
        "test_x_lem = pd.DataFrame()\n",
        "test_x_lem['title'] = X_test['Title no stopwords8punc8dig'].apply(lemmatize_text)\n",
        "test_x_lem['description'] = X_test['Description no stopwords8punc8dig'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "GN-f3BcxcSMv"
      },
      "id": "GN-f3BcxcSMv",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stemming(txt):\n",
        "    stem_words = [stemmer.stem(w) for w in txt.split()]\n",
        "    return stem_words # ' '.join(stem_words)\n",
        "\n",
        "# Применим стемминг к каждому столбцу в тест и трейн\n",
        "train_x_stem = pd.DataFrame()\n",
        "train_x_stem['title'] = X_train['Title no stopwords8punc8dig'].apply(stemming)\n",
        "train_x_stem['description'] = X_train['Description no stopwords8punc8dig'].apply(stemming)\n",
        "\n",
        "test_x_stem = pd.DataFrame()\n",
        "test_x_stem['title'] = X_test['Title no stopwords8punc8dig'].apply(stemming)\n",
        "test_x_stem['description'] = X_test['Description no stopwords8punc8dig'].apply(stemming)"
      ],
      "metadata": {
        "id": "WQImfngcZ1uw"
      },
      "id": "WQImfngcZ1uw",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Итого два варианта: train_x_lem, test_x_lem и train_x_stem,test_x_stem\n",
        "# Which is improve model quality better узнаем только по модели, поэтому дальше используем оба набора данных"
      ],
      "metadata": {
        "id": "_YqJW_WEh0u5"
      },
      "id": "_YqJW_WEh0u5",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x_lem.head(3))\n",
        "print(train_x_stem.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE2j768Ajw4Z",
        "outputId": "8698bf7c-c0f9-4e36-87fa-1824ce82c0c1"
      },
      "id": "WE2j768Ajw4Z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0       [wall, st, bear, claw, back, black, reuters]   \n",
            "1  [carlyle, look, toward, commercial, aerospace,...   \n",
            "2     [oil, economy, cloud, stock, outlook, reuters]   \n",
            "\n",
            "                                         description  \n",
            "0  [reuters, shortsellers, wall, street, dwindlin...  \n",
            "1  [reuters, private, investment, firm, carlyle, ...  \n",
            "2  [reuters, soaring, crude, price, plus, worries...  \n",
            "                                               title  \\\n",
            "0        [wall, st, bear, claw, back, black, reuter]   \n",
            "1  [carlyl, look, toward, commerci, aerospac, reu...   \n",
            "2      [oil, economi, cloud, stock, outlook, reuter]   \n",
            "\n",
            "                                         description  \n",
            "0  [reuter, shortsel, wall, street, dwindlingband...  \n",
            "1  [reuter, privat, invest, firm, carlyl, groupwh...  \n",
            "2  [reuter, soar, crude, price, plus, worriesabou...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "_v7a6zVlYuIV"
      },
      "id": "_v7a6zVlYuIV",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_tfidf = tfidf_vectorizer.fit_transform(X_train['Title no stopwords8punc8dig'])\n",
        "title_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "title_train_tfidf = pd.DataFrame(title_tfidf.toarray(), columns = title_names_tfidf)\n",
        "\n",
        "desc_tfidf = tfidf_vectorizer.fit_transform(X_train['Description no stopwords8punc8dig'])\n",
        "desc_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "desc_train_tfidf = pd.DataFrame(desc_tfidf.toarray(), columns = desc_names_tfidf)"
      ],
      "metadata": {
        "id": "mDljjwhijyYm"
      },
      "id": "mDljjwhijyYm",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_test_tfidf = tfidf_vectorizer.transform(X_test['Title no stopwords8punc8dig'])\n",
        "title_test_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "title_test_tfidf = pd.DataFrame(title_test_tfidf.toarray(), columns = title_test_names_tfidf)\n",
        "\n",
        "desc_test_tfidf = tfidf_vectorizer.transform(X_test['Description no stopwords8punc8dig'])\n",
        "desc_test_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "desc_test_tfidf = pd.DataFrame(desc_test_tfidf.toarray(), columns = desc_test_names_tfidf)"
      ],
      "metadata": {
        "id": "xXW5dnmOQM43"
      },
      "id": "xXW5dnmOQM43",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_title = gensim.models.Word2Vec(X_train['Title no stopwords8punc8dig'], min_count=1,\n",
        "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
        "word2vec_desc = gensim.models.Word2Vec(X_train['Description no stopwords8punc8dig'], min_count=1,\n",
        "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
        "word2vec_test_title = gensim.models.Word2Vec(X_test['Title no stopwords8punc8dig'], min_count=1,\n",
        "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
        "word2vec_test_desc = gensim.models.Word2Vec(X_train['Description no stopwords8punc8dig'], min_count=1,\n",
        "\t\t\t\t\t\t\t\tvector_size=100, window=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vrw2qK3YUGw",
        "outputId": "22b6a91d-582e-4c70-f0af-917759f3f03b"
      },
      "id": "0vrw2qK3YUGw",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSjCw8KAan8y",
        "outputId": "18dbdf95-fb68-45a8-b3dc-2c961575bf38"
      },
      "id": "NSjCw8KAan8y",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect_transformers_title = transformer.encode(X_train['Title no stopwords8punc8dig'])\n",
        "vect_transformers_desc = transformer.encode(X_train['Description no stopwords8punc8dig'])\n",
        "vect_transformers_test_title = transformer.encode(X_test['Title no stopwords8punc8dig'])\n",
        "vect_transformers_test_desc = transformer.encode(X_test['Description no stopwords8punc8dig'])"
      ],
      "metadata": {
        "id": "9VXGgHUTjs6z"
      },
      "id": "9VXGgHUTjs6z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}