{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NikolayKuraga/ML_lab_2/blob/master/2_nn_text_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sentence-transformers -q"
   ],
   "metadata": {
    "id": "vt738_1Qi4Nd"
   },
   "id": "vt738_1Qi4Nd",
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee1b0f88-b8c8-4b31-aec2-575ffe7b387b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee1b0f88-b8c8-4b31-aec2-575ffe7b387b",
    "outputId": "dce03928-adc7-4afe-b0a5-a098acf27c66"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WOW! I can execute shell commands from Jupyter Notebook!\n",
      "Interesting, what shell is used under Windows...\n",
      "Current shell is: \"/bin/bash\".\n",
      "Done\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!echo 'WOW! I can execute shell commands from Jupyter Notebook!'\n",
    "!echo 'Interesting, what shell is used under Windows...'\n",
    "!echo \"Current shell is: \\\"${SHELL}\\\".\"\n",
    "!pip install fast_langdetect iso-639 nltk pandas >/dev/null\n",
    "\n",
    "\n",
    "import enum\n",
    "import string\n",
    "import typing as t\n",
    "\n",
    "\n",
    "import fast_langdetect\n",
    "import iso639\n",
    "import nltk.corpus\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3519c42-4cca-439e-a268-f5ba24cbd1c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3519c42-4cca-439e-a268-f5ba24cbd1c0",
    "outputId": "1c16533d-7bfb-4095-9a8b-e1422b04211d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Head of train dataset:\n",
      "   Class Index                                              Title  \\\n",
      "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
      "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
      "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
      "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
      "4            3  Oil prices soar to all-time record, posing new...   \n",
      "\n",
      "                                         Description  \n",
      "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
      "1  Reuters - Private investment firm Carlyle Grou...  \n",
      "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
      "3  Reuters - Authorities have halted oil export\\f...  \n",
      "4  AFP - Tearaway world oil prices, toppling reco...  \n",
      "\n",
      "Info of train dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Class Index  120000 non-null  int64 \n",
      " 1   Title        120000 non-null  object\n",
      " 2   Description  120000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv('https://raw.githubusercontent.com/NikolayKuraga/ML_lab_2/refs/heads/master/train.csv')\n",
    "df_tst = pd.read_csv('https://raw.githubusercontent.com/NikolayKuraga/ML_lab_2/refs/heads/master/test.csv')\n",
    "\n",
    "print('Head of train dataset:')\n",
    "print(df_trn.head())\n",
    "print()\n",
    "print('Info of train dataset')\n",
    "print(df_trn.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Try some text cleaning techniques from the list below (at least 2). Does any of it improved your model quality? Try to assume why."
   ],
   "metadata": {
    "id": "jtC5UmmyYyoe"
   },
   "id": "jtC5UmmyYyoe"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dd55ef6-32a6-4b52-a725-43c656d74377",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dd55ef6-32a6-4b52-a725-43c656d74377",
    "outputId": "35c8207b-f115-4a13-a1ce-38d59f28305c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train dataset\n",
      "Lang\n",
      "english           119944\n",
      "french                21\n",
      "spanish               10\n",
      "polish                 9\n",
      "german                 7\n",
      "italian                5\n",
      "westernfrisian         3\n",
      "ukrainian              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test dataset\n",
      "Lang\n",
      "english    7596\n",
      "polish        3\n",
      "french        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Try some text cleaning techniques from the list below (at least 2). Does any of it\n",
    "# improved your model quality? Try to assume why.\n",
    "#     a. stop words removing\n",
    "#     b. punctuation removing\n",
    "#     c. Trash (Extra spaces / special symbols (like @#~<> etc)) removing\n",
    "#     d. digits removing\n",
    "\n",
    "\n",
    "# First of all tried to detect what languages are used.\n",
    "# Found two ways:\n",
    "#     1) langdetect -- \"traditional\" module, really slow (several seconds for 1000 entities).\n",
    "#     2) fast_langdetect -- fast \"zoomer\" language-machine-learning based module.\n",
    "# I will use fast_langdetect model 'cause I'm zoomer.\n",
    "\n",
    "def detectLanguage(text: str) -> str:\n",
    "    code = fast_langdetect.detect_language(text).lower()\n",
    "    name = iso639.languages.get(alpha2=code).name\n",
    "    name = ''.join(name.split()).lower()\n",
    "    return name\n",
    "\n",
    "\n",
    "df_trn['Lang'] = (df_trn['Title'] + ' ' + df_trn['Description']).apply(detectLanguage)\n",
    "df_tst['Lang'] = (df_tst['Title'] + ' ' + df_tst['Description']).apply(detectLanguage)\n",
    "\n",
    "print('Train dataset')\n",
    "print(df_trn['Lang'].value_counts())\n",
    "print()\n",
    "print('Test dataset')\n",
    "print(df_tst['Lang'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4306691c-7a1a-4a0f-859c-3d9d47da3e39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4306691c-7a1a-4a0f-859c-3d9d47da3e39",
    "outputId": "37226748-af6b-49b5-d48e-0800c167f559"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no \"polish\" language in stopwords database?\n",
      "no \"ukrainian\" language in stopwords database?\n",
      "no \"westernfrisian\" language in stopwords database?\n",
      "\n",
      "Got so many (1089) stopwords from different languages!\n",
      "\n",
      "35                                  Steady as they go\n",
      "36         Google IPO: Type in 'confusing,' 'secrecy'\n",
      "37                        A bargain hunter's paradise\n",
      "38     Researchers seek to untangle the e-mail thread\n",
      "39    Microsoft Corp. 2.0: a kinder corporate culture\n",
      "Name: Title, dtype: object\n",
      "\n",
      "35                                     steady go\n",
      "36             google ipo type confusing secrecy\n",
      "37                      bargain hunters paradise\n",
      "38        researchers seek untangle email thread\n",
      "39    microsoft corp 20 kinder corporate culture\n",
      "Name: Title no stopwords8punc, dtype: object\n",
      "\n",
      "35                                  steady go\n",
      "36          google ipo type confusing secrecy\n",
      "37                   bargain hunters paradise\n",
      "38     researchers seek untangle email thread\n",
      "39    microsoft corp kinder corporate culture\n",
      "Name: Title no stopwords8punc8dig, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find necessary stopwords in nltk database.\n",
    "stopwords = set()\n",
    "for lang in df_trn['Lang'].unique():\n",
    "    try:\n",
    "        stopwords |= set(nltk.corpus.stopwords.words(lang))\n",
    "    except:\n",
    "        print(f'no \"{lang}\" language in stopwords database?')\n",
    "\n",
    "print()\n",
    "print(f'Got so many ({len(stopwords)}) stopwords from different languages!')\n",
    "\n",
    "def cleanTextFromStopwords8Punc(txt: str) -> str:\n",
    "    txt = ''.join([sym for sym in txt if sym not in string.punctuation])\n",
    "    txt = ' '.join([wrd for wrd in txt.lower().split() if wrd not in stopwords])\n",
    "    return txt\n",
    "\n",
    "def cleanTextFromStopwords8Punc8Dig(txt: str) -> str:\n",
    "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
    "    txt = ''.join([sym for sym in txt if sym not in string.punctuation])\n",
    "    txt = ' '.join([wrd for wrd in txt.lower().split() if wrd not in stopwords])\n",
    "    return txt\n",
    "\n",
    "df_trn['Title no stopwords8punc'] = df_trn['Title'].apply(cleanTextFromStopwords8Punc)\n",
    "df_trn['Title no stopwords8punc8dig'] = df_trn['Title'].apply(cleanTextFromStopwords8Punc8Dig) #Title no stopwords8punc8dig\n",
    "df_trn['Description no stopwords8punc'] = df_trn['Description'].apply(cleanTextFromStopwords8Punc) #Description no stopwords8punc\n",
    "df_trn['Description no stopwords8punc8dig'] = df_trn['Description'].apply(cleanTextFromStopwords8Punc8Dig)\n",
    "\n",
    "df_tst['Title no stopwords8punc'] = df_tst['Title'].apply(cleanTextFromStopwords8Punc)\n",
    "df_tst['Title no stopwords8punc8dig'] = df_tst['Title'].apply(cleanTextFromStopwords8Punc8Dig)\n",
    "df_tst['Description no stopwords8punc'] = df_tst['Description'].apply(cleanTextFromStopwords8Punc)\n",
    "df_tst['Description no stopwords8punc8dig'] = df_tst['Description'].apply(cleanTextFromStopwords8Punc8Dig)\n",
    "\n",
    "print()\n",
    "print(df_trn['Title'].iloc[35:40])\n",
    "print()\n",
    "print(df_trn['Title no stopwords8punc'].iloc[35:40])\n",
    "print()\n",
    "print(df_trn['Title no stopwords8punc8dig'].iloc[35:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Try to apply stemming and lemmatization."
   ],
   "metadata": {
    "id": "avgumQcoY3Mt"
   },
   "id": "avgumQcoY3Mt"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train =  df_trn[['Title no stopwords8punc8dig','Description no stopwords8punc8dig']]\n",
    "y_train =  df_trn['Class Index']\n",
    "\n",
    "X_test =  df_tst[['Title no stopwords8punc8dig','Description no stopwords8punc8dig']]"
   ],
   "metadata": {
    "id": "kZm4glhaYa7A"
   },
   "id": "kZm4glhaYa7A",
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatzation(lst):\n",
    "   new_lem = []\n",
    "   for i in lst:\n",
    "       i = lemmatizer.lemmatize(i)\n",
    "       new_lem.append(i)\n",
    "   return new_lem\n",
    "\n",
    "train_x_lem = X_train.apply(lemmatzation)\n",
    "test_x_lem = X_test.apply(lemmatzation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCxnr7iTYtKm",
    "outputId": "c5bcf1f3-fda3-4157-ace3-589517d90beb"
   },
   "id": "iCxnr7iTYtKm",
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def lemmatize_text(txt):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in txt.split()]  # Лемматизация\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "# Применим лемматизацию к каждому столбцу в тест и трейн\n",
    "train_x_lem = pd.DataFrame()\n",
    "train_x_lem['title'] = X_train['Title no stopwords8punc8dig'].apply(lemmatize_text)\n",
    "train_x_lem['description'] = X_train['Description no stopwords8punc8dig'].apply(lemmatize_text)\n",
    "\n",
    "test_x_lem = pd.DataFrame()\n",
    "test_x_lem['title'] = X_test['Title no stopwords8punc8dig'].apply(lemmatize_text)\n",
    "test_x_lem['description'] = X_test['Description no stopwords8punc8dig'].apply(lemmatize_text)"
   ],
   "metadata": {
    "id": "GN-f3BcxcSMv"
   },
   "id": "GN-f3BcxcSMv",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(txt):\n",
    "    stem_words = [stemmer.stem(w) for w in txt.split()]\n",
    "    return stem_words # ' '.join(stem_words)\n",
    "\n",
    "# Применим стемминг к каждому столбцу в тест и трейн\n",
    "train_x_stem = pd.DataFrame()\n",
    "train_x_stem['title'] = X_train['Title no stopwords8punc8dig'].apply(stemming)\n",
    "train_x_stem['description'] = X_train['Description no stopwords8punc8dig'].apply(stemming)\n",
    "\n",
    "test_x_stem = pd.DataFrame()\n",
    "test_x_stem['title'] = X_test['Title no stopwords8punc8dig'].apply(stemming)\n",
    "test_x_stem['description'] = X_test['Description no stopwords8punc8dig'].apply(stemming)"
   ],
   "metadata": {
    "id": "WQImfngcZ1uw"
   },
   "id": "WQImfngcZ1uw",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Итого два варианта: train_x_lem, test_x_lem и train_x_stem,test_x_stem\n",
    "# Which is improve model quality better узнаем только по модели, поэтому дальше используем оба набора данных"
   ],
   "metadata": {
    "id": "_YqJW_WEh0u5"
   },
   "id": "_YqJW_WEh0u5",
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(train_x_lem.head(3))\n",
    "print(train_x_stem.head(3))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WE2j768Ajw4Z",
    "outputId": "3884eb21-1a4a-49dd-cf17-201a1866f89b"
   },
   "id": "WE2j768Ajw4Z",
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                               title  \\\n",
      "0       [wall, st, bear, claw, back, black, reuters]   \n",
      "1  [carlyle, look, toward, commercial, aerospace,...   \n",
      "2     [oil, economy, cloud, stock, outlook, reuters]   \n",
      "\n",
      "                                         description  \n",
      "0  [reuters, shortsellers, wall, street, dwindlin...  \n",
      "1  [reuters, private, investment, firm, carlyle, ...  \n",
      "2  [reuters, soaring, crude, price, plus, worries...  \n",
      "                                               title  \\\n",
      "0        [wall, st, bear, claw, back, black, reuter]   \n",
      "1  [carlyl, look, toward, commerci, aerospac, reu...   \n",
      "2      [oil, economi, cloud, stock, outlook, reuter]   \n",
      "\n",
      "                                         description  \n",
      "0  [reuter, shortsel, wall, street, dwindlingband...  \n",
      "1  [reuter, privat, invest, firm, carlyl, groupwh...  \n",
      "2  [reuter, soar, crude, price, plus, worriesabou...  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#3. Apply vectorization"
   ],
   "metadata": {
    "id": "CrMbom-berox"
   },
   "id": "CrMbom-berox"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Предполагаем, что y_train — это список или numpy массив, соответствующий размеру train_x_lem['description']\n",
    "\n",
    "# Фильтрация train_x_lem['description'] с сохранением индексной информации\n",
    "filtered_data = [(i, x) for i, x in enumerate(train_x_lem['description']) if len(x) > 1]\n",
    "\n",
    "# Извлечение списка элементов и соответствующих им индексов\n",
    "filtered_train_x_lem_for_vect = [x for _, x in filtered_data]\n",
    "filtered_indices = [i for i, _ in filtered_data]\n",
    "\n",
    "# Фильтрация y_train по сохраненным индексам\n",
    "filtered_y_train = [y_train[i] for i in filtered_indices]\n",
    "\n",
    "# Теперь у вас есть оба фильтрованных списка\n",
    "train_x_lem_for_vect = filtered_train_x_lem_for_vect\n",
    "y_train_lem_for_vect = filtered_y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# train_x_lem_for_vect = [x for x in train_x_lem['description'] if len(x) > 1]\n",
    "test_x_lem_for_vect = [x for x in test_x_lem['description'] if len(x) > 1]"
   ],
   "metadata": {
    "id": "22irWM9K9783"
   },
   "id": "22irWM9K9783",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_x_stem_for_vect = [x for x in train_x_stem['description'] if len(x) > 1]\n",
    "test_x_stem_for_vect = [x for x in test_x_stem['description'] if len(x) > 1]"
   ],
   "metadata": {
    "id": "X4yChz3--QIb"
   },
   "id": "X4yChz3--QIb",
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ],
   "metadata": {
    "id": "_v7a6zVlYuIV"
   },
   "id": "_v7a6zVlYuIV",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "desc_tfidf_lem = [tfidf_vectorizer.fit_transform(x) for x in train_x_lem_for_vect]\n",
    "desc_test_tfidf_lem = [tfidf_vectorizer.transform(x) for x in test_x_lem_for_vect]\n"
   ],
   "metadata": {
    "id": "mDljjwhijyYm"
   },
   "id": "mDljjwhijyYm",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "desc_tfidf_stem = [tfidf_vectorizer.fit_transform(x) for x in train_x_stem_for_vect]\n",
    "desc_test_tfidf_stem = [tfidf_vectorizer.transform(x) for x in test_x_stem_for_vect]"
   ],
   "metadata": {
    "id": "xXW5dnmOQM43"
   },
   "id": "xXW5dnmOQM43",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word2vec_desc_lem = gensim.models.Word2Vec(train_x_lem_for_vect, min_count=1,\n",
    "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
    "word2vec_test_desc_lem = gensim.models.Word2Vec(test_x_lem_for_vect, min_count=1,\n",
    "\t\t\t\t\t\t\t\tvector_size=100, window=5)"
   ],
   "metadata": {
    "id": "0vrw2qK3YUGw"
   },
   "id": "0vrw2qK3YUGw",
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word2vec_desc_stem = gensim.models.Word2Vec(train_x_stem_for_vect, min_count=1,\n",
    "\t\t\t\t\t\t\t\tvector_size=100, window=5)\n",
    "word2vec_test_desc_stem = gensim.models.Word2Vec(test_x_stem_for_vect, min_count=1,\n",
    "\t\t\t\t\t\t\t\tvector_size=100, window=5)"
   ],
   "metadata": {
    "id": "4NIuaFINS5i8"
   },
   "id": "4NIuaFINS5i8",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transformer = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSjCw8KAan8y",
    "outputId": "5e16f697-df63-4065-fe44-abc84acc6e1b"
   },
   "id": "NSjCw8KAan8y",
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vect_transformers_desc_lem = transformer.encode(train_x_lem_for_vect)\n",
    "vect_transformers_test_desc_lem = transformer.encode(test_x_lem_for_vect)"
   ],
   "metadata": {
    "id": "9VXGgHUTjs6z"
   },
   "id": "9VXGgHUTjs6z",
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vect_transformers_desc_stem = transformer.encode(train_x_stem_for_vect)\n",
    "vect_transformers_test_desc_stem = transformer.encode(test_x_stem_for_vect)"
   ],
   "metadata": {
    "id": "yABkrHoZTUJU"
   },
   "id": "yABkrHoZTUJU",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Импорт EarlyStopping\n",
    "\n",
    "# Предполагаемая форма данных: (batch_size, sequence_length, features)\n",
    "# Например, если ваши векторы имеют 384 признаков\n",
    "input_shape = (384, 1)\n",
    "\n",
    "# Создание модели CNN\n",
    "modelCNN = models.Sequential()\n",
    "\n",
    "# Первый свёрточный слой для одномерных данных\n",
    "modelCNN.add(layers.Conv1D(128, 3, activation='relu', input_shape=input_shape))\n",
    "\n",
    "# Второй свёрточный слой\n",
    "modelCNN.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "modelCNN.add(layers.MaxPooling1D(2))\n",
    "\n",
    "# Полносвязные слои\n",
    "modelCNN.add(layers.Flatten())\n",
    "modelCNN.add(layers.Dense(64, activation='relu'))\n",
    "modelCNN.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Компиляция модели\n",
    "modelCNN.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Преобразование векторных меток в формат one-hot\n",
    "y_train_categorical = to_categorical(y_train_lem_for_vect, num_classes=10)\n",
    "\n",
    "# Нормализация вектора входных данных\n",
    "vect_transformers_desc_lem_norm = vect_transformers_desc_lem.reshape((vect_transformers_desc_lem.shape[0], 384, 1))\n",
    "\n",
    "# Установка EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели\n",
    "modelCNN.fit(vect_transformers_desc_lem_norm, y_train_categorical, epochs=10, callbacks=[early_stopping])\n"
   ],
   "metadata": {
    "id": "RPPt2Y2o-we-"
   },
   "id": "RPPt2Y2o-we-",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Нормализация вектора входных данных\n",
    "vect_transformers_desc_lem_norm = vect_transformers_test_desc_lem.reshape((vect_transformers_test_desc_lem.shape[0], 384, 1))\n",
    "\n",
    "predictions = modelCNN.predict(vect_transformers_desc_lem_norm)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "ids = np.arange(len(predicted_classes))\n",
    "df_results = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Class Index': predicted_classes\n",
    "})\n",
    "\n",
    "df_results.to_csv('predictionsCNN.csv', index=False)\n",
    "print(\"Предсказания сохранены в 'predictions.csv'\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
